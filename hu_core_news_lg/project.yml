title: "Part-of-speech Tagging, Lemmatization & Dependency Parsing (Universal Dependencies)"
description: ""
spacy_version: ">=3.1.0,<3.2.0"

vars:
  lang: hu
  # The convention is to use the format: modeltype_preferreddomain_modelsize
  core_package_name: core_news_lg
  # Wrokaround: spacy project yaml cannot interpolate variables inside the `vars` sections
  package_name: hu_core_news_lg
  package_version: 0.1.0

  models_path: ../models
  packages_path: ../packages

  raw_data_path: ../data/raw
  processed_data_path: ../data/processed
  external_data_path: ../data/external

  treebank: UD_Hungarian-Szeged
  train_name: hu_szeged-ud-train
  dev_name: hu_szeged-ud-dev
  test_name: hu_szeged-ud-test
  nerkor: NerKor

  package_init: ../huspacy/components.py

  wandb_entity: spacy-hu
  wandb_project: hu_core_news_lg

  assemble_config: assemble
  parser_config: parser
  ner_config: ner
  gpu: 0

#TODO: remove GPU settings
#TODO: provide examples for WANDB and GPU settings

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories: [configs, ../models, ../packages, ../data/raw, ../data/processed, ../data/external]

assets:
  - dest: ${vars.raw_data_path}/${vars.treebank}
    git:
      repo: https://github.com/UniversalDependencies/${vars.treebank}
      branch: master
      path: ""

  - dest: ${vars.raw_data_path}/${vars.nerkor}
    git:
      repo: https://github.com/nytud/NYTK-NerKor
      branch: main
      path: data

  - dest: ${vars.external_data_path}/webcorpuswiki.word2vec.bz2
    url: https://github.com/oroszgy/hunlp-resources/releases/download/webcorpuswiki_word2vec_v0.1/webcorpuswiki.word2vec.bz2


workflows:
  all:
    - init_vectors
    - preprocess_ud
    - preprocess_nerkor
    - train_lemmatizer
    - train_parser
    - train_ner
    - assemble
    - evaluate
    - package
    - publish

commands:
  - name: init_vectors
    help: "Initialize vectors"
    script:
      - "bash -c '/bin/bzcat ${vars.external_data_path}/webcorpuswiki.word2vec.bz2
          > ${vars.external_data_path}/webcorpuswiki.word2vec.txt'"
      #TODO: store the vectors in the models folder
      - "python -m spacy init vectors hu ${vars.external_data_path}/webcorpuswiki.word2vec.txt
          ${vars.external_data_path}/webcorpuswiki.word2vec.vec
          -n ${vars.package_name}.vectors"
    deps:
      - ${vars.external_data_path}/webcorpuswiki.word2vec.bz2
    outputs:
      - ${vars.external_data_path}/webcorpuswiki.word2vec.vec

  - name: preprocess_ud
    help: "Convert the UD corpus to spaCy's format"
    script:
      - "mkdir -p ${vars.processed_data_path}/${vars.treebank}"
      - "python -m spacy convert ${vars.raw_data_path}/${vars.treebank}/${vars.train_name}.conllu
          ${vars.processed_data_path}/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens"
      - "python -m spacy convert ${vars.raw_data_path}/${vars.treebank}/${vars.dev_name}.conllu
          ${vars.processed_data_path}/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens"
      - "python -m spacy convert ${vars.raw_data_path}/${vars.treebank}/${vars.test_name}.conllu
          ${vars.processed_data_path}/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens"
      - "mv ${vars.processed_data_path}/${vars.treebank}/${vars.train_name}.spacy
          ${vars.processed_data_path}/${vars.treebank}/train.spacy"
      - "mv ${vars.processed_data_path}/${vars.treebank}/${vars.dev_name}.spacy
          ${vars.processed_data_path}/${vars.treebank}/dev.spacy"
      - "mv ${vars.processed_data_path}/${vars.treebank}/${vars.test_name}.spacy
          ${vars.processed_data_path}/${vars.treebank}/test.spacy"
    deps:
      - ${vars.raw_data_path}/${vars.treebank}/${vars.train_name}.conllu
      - ${vars.raw_data_path}/${vars.treebank}/${vars.dev_name}.conllu
      - ${vars.raw_data_path}/${vars.treebank}/${vars.test_name}.conllu
    outputs:
      - ${vars.processed_data_path}/${vars.treebank}/train.spacy
      - ${vars.processed_data_path}/${vars.treebank}/dev.spacy
      - ${vars.processed_data_path}/${vars.treebank}/test.spacy

  - name: preprocess_nerkor
    help: "Convert the UD corpus to spaCy's format"
    script:
      - "mkdir -p ${vars.processed_data_path}/${vars.nerkor}"
      - "convert2conllu '${vars.raw_data_path}/${vars.nerkor}/train-devel-test/devel/*/morph/*.conllup'
          ${vars.processed_data_path}/${vars.nerkor}/dev.conllup"
      - "convert2conllu '${vars.raw_data_path}/${vars.nerkor}/train-devel-test/train/*/morph/*.conllup'
          ${vars.processed_data_path}/${vars.nerkor}/train.conllup"
      - "convert2conllu '${vars.raw_data_path}/${vars.nerkor}/train-devel-test/test/*/morph/*.conllup'
          ${vars.processed_data_path}/${vars.nerkor}/test.conllup"

      - python -m spacy convert ${vars.processed_data_path}/${vars.nerkor}/train.conllup ${vars.processed_data_path}/${vars.nerkor} --converter conllu --n-sents 10
      - python -m spacy convert ${vars.processed_data_path}/${vars.nerkor}/dev.conllup ${vars.processed_data_path}/${vars.nerkor} --converter conllu --n-sents 10
      - python -m spacy convert ${vars.processed_data_path}/${vars.nerkor}/test.conllup ${vars.processed_data_path}/${vars.nerkor} --converter conllu --n-sents 10
    deps:
      - ${vars.raw_data_path}/${vars.nerkor}
    outputs:
      - ${vars.processed_data_path}/${vars.nerkor}/train.conllup
      - ${vars.processed_data_path}/${vars.nerkor}/dev.conllup
      - ${vars.processed_data_path}/${vars.nerkor}/test.conllup
      - ${vars.processed_data_path}/${vars.nerkor}/train.spacy
      - ${vars.processed_data_path}/${vars.nerkor}/dev.spacy
      - ${vars.processed_data_path}/${vars.nerkor}/test.spacy

  - name: train_lemmatizer
    help: "Train the lemmatizer"
    script:
      - "lemmy train ${vars.processed_data_path}/${vars.nerkor}/train.conllup ${vars.models_path}/lemmy-${vars.package_version}.bin"
      - "lemmy evaluate ${vars.models_path}/lemmy-${vars.package_version}.bin ${vars.processed_data_path}/${vars.nerkor}/test.conllup"
      - "lemmy evaluate ${vars.models_path}/lemmy-${vars.package_version}.bin ${vars.raw_data_path}/${vars.treebank}/${vars.test_name}.conllu"
    deps:
      - ${vars.processed_data_path}/${vars.nerkor}/train.conllup
      - ${vars.processed_data_path}/${vars.nerkor}/test.conllup
      - ${vars.raw_data_path}/${vars.treebank}/${vars.test_name}.conllu
    outputs:
      - ${vars.models_path}/lemmy-${vars.package_version}.bin

  - name: train_parser
    help: "Train the parser/tagger"
    script:
      - "python -m spacy train configs/${vars.parser_config}.cfg
            --output ${vars.models_path}/${vars.package_name}-parser-${vars.package_version}
            --gpu-id ${vars.gpu} --nlp.lang=${vars.lang}
            --paths.vectors ${vars.external_data_path}/webcorpuswiki.word2vec.vec
            --paths.train ${vars.processed_data_path}/${vars.treebank}/train.spacy
            --paths.dev ${vars.processed_data_path}/${vars.treebank}/dev.spacy"
    deps:
      - ${vars.processed_data_path}/${vars.treebank}/train.spacy
      - ${vars.processed_data_path}/${vars.treebank}/dev.spacy
      - configs/${vars.parser_config}.cfg
    outputs:
      - ${vars.models_path}/${vars.package_name}-parser-${vars.package_version}/model-best

  - name: train_ner
    help: "Train the NER model"
    script:
      - "python -m spacy train configs/${vars.ner_config}.cfg
          --output ${vars.models_path}/${vars.package_name}-ner-${vars.package_version}
          --nlp.lang=${vars.lang} --gpu-id ${vars.gpu}
          --paths.vectors ${vars.external_data_path}/webcorpuswiki.word2vec.vec
          --paths.train ${vars.processed_data_path}/${vars.nerkor}/train.spacy
          --paths.dev ${vars.processed_data_path}/${vars.nerkor}/dev.spacy"
    deps:
      - ${vars.processed_data_path}/${vars.nerkor}/train.spacy
      - ${vars.processed_data_path}/${vars.nerkor}/dev.spacy
      - ${vars.processed_data_path}/${vars.nerkor}/test.spacy
    outputs:
      - ${vars.models_path}/${vars.package_name}-ner-${vars.package_version}/model-best

  - name: assemble
    help: "Assemble the parser, the lemmatizer and the NER components"
    script:
      - "spacy assemble configs/${vars.assemble_config}.cfg
      -c ${vars.package_init}
      -VV
      ${vars.models_path}/${vars.package_name}-${vars.package_version}"
    deps:
      - ${vars.models_path}/${vars.package_name}-parser-${vars.package_version}/model-best
      - ${vars.models_path}/${vars.package_name}-ner-${vars.package_version}/model-best
      - ${vars.package_init}
    outputs:
      - ${vars.models_path}/${vars.package_name}-${vars.package_version}/model-best

  - name: package
    help: "Package the trained model so it can be installed"
    script: #TODO: add meta
      - "python -m spacy package
          ${vars.models_path}/${vars.package_name}-${vars.package_version} ${vars.packages_path}
          --build 'sdist, wheel'
          --code ${vars.package_init}
          --name ${vars.core_package_name}
          --version ${vars.package_version}
          --force"
    deps:
      -  ${vars.models_path}/${vars.package_name}-${vars.package_version}
      - ${vars.package_init}
    outputs_no_cache:
      - ${vars.packages_path}/${vars.package_name}-${vars.package_version}/dist/en_${vars.package_name}-${vars.package_version}.tar.gz
      - ${vars.packages_path}/${vars.package_name}-${vars.package_version}/dist/en_${vars.package_name}-${vars.package_version}-py3-none-any.whl

  - name: evaluate
    help: "Evaluate on the test data and save the metrics"
    script:
      - "python -m spacy evaluate
          ${vars.models_path}/${vars.package_name}-${vars.package_version}/model-best
          ${vars.processed_data_path}/${vars.treebank}/test.spacy
          --output ${vars.models_path}/${vars.package_name}-${vars.package_version}.json --gpu-id ${vars.gpu} -G"
    deps:
      - ${vars.models_path}/${vars.package_name}-${vars.package_version}/model-best
      - ${vars.processed_data_path}/${vars.treebank}/test.spacy
    outputs:
      - "metrics/${vars.treebank}.json"

  - name: publish
    help: "Upload the trained model to the Hugging Face Hub"
    script:
      - "python -m spacy huggingface-hub push
          ${vars.packages_path}/${vars.package_name}-${vars.package_version}/dist/${vars.package_name}-${vars.package_version}-py3-none-any.whl"
    deps:
      - ${vars.packages_path}/${vars.package_name}-${vars.package_version}/dist/${vars.package_name}-${vars.package_version}-py3-none-any.whl

  - name: clean
    help: "Remove intermediate files"
    script:
      - "rm -rf ${vars.processed_data_path}"
      - "rm -rf ${vars.models_path}"
